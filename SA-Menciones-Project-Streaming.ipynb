{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://qs.topuniversities.com/hs-fs/hubfs/07_HumanScience_H_300.jpg?width=4961&height=3508&name=07_HumanScience_H_300.jpg\" width=\"250\" height=\"100\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk The Talk || Hands-On Big Data Streaming using Apache Spark \n",
    "## Real-Time Analysis of Your Political Standing\n",
    "### Group F\n",
    "#### Nicolas Fraire, Camille Eloi, IÃ±igo Hidalgo, Dea Markovic, Harshil Sharma, Oriol Vila, Gerardo Gandara\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the amazing frameworks that can handle big data in real-time and perform different analysis, is Apache Spark. I think there are many resources that provide information about different features of Spark and how popular it is in the big data community but shortly mentioning the core features of Spark: it does fast big data processing employing Resilient Distributed Datasets (RDDs), streaming and Machine learning on a large scale at real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/07/performing-twitter-sentiment-analysis1.jpg" />\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade pip\n",
    "#!pip3 install pysentimiento\n",
    "#!pip3 install torch\n",
    "#!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentimiento import SentimentAnalyzer\n",
    "analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We are going to use findspark library to locate Spark on our local machine and then we import necessary packages from pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you need to put where is spark installed\n",
    "# with this command : echo 'sc.getConfget('spark.home')' | spark-shell\n",
    "findspark.init('/opt/spark-3.0.0-bin-hadoop3.2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May cause deprecation warnings, safe to ignore, they aren't errors\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We initiate SparkContext(). SparkContext is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster. SparkContext here uses Py4J to launch a JVM and creates a JavaSparkContext (source). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can only run this once. restart your kernel for any errors.\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is important to note that **only one SparkContext** may be running at each session. (That is why the trial and error is time consuming when you are learning, but it is ok, it is worthy!).\n",
    "\n",
    "After that, we initiate the StreamingContext() with 10-second batch intervals, it means that the input streams will be divided into batches every 10 seconds during the streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10 )\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 5555)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.checkpoint(\"checkpoint_TwitterApp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, our next step is to assign our input source of streaming and then put the incoming data in variable My_READ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is important to note that we are using the same port number (5555) as we used in the first module to send the tweets and the IP address (VM for our case) is the same since we are running things on our local machine. in addition\n",
    "\n",
    "**We are using the window() function to determine that we are analyzing tweets every minute (60 seconds) to see what the top 10 #tags are during that time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_READ = socket_stream.window( 60 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will create TEMP tables for each candidate/mentions and a table with TWEETS for SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politicians Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we need to calculate how many times the Candidate has been mentioned. We can do that by using the function reduceByKey. This function will calculate how many times the hashtag has been mentioned per each batch, i.e. it will reset the counts in each batch. In this version for prototype purposes we used reduceByKey but it important to emphasize the difference. We can simulate it with a loop calculation later for you to see the difference.\n",
    "\n",
    "#### In our case, we need to calculate the counts across all the batches, so weâ€™ll use another function called updateStateByKey, as this function allows you to maintain the state of RDD while updating it with new data. This way is called Stateful Transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( My_READ.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"ayuso\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 key words that starts with the candidate name to a table.\n",
    "  .limit(10).registerTempTable(\"c_ayuso\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( My_READ.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"gabilondo\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 candidate to a table.\n",
    "  .limit(10).registerTempTable(\"c_gabilondo\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( My_READ.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"monica\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 candidate to a table.\n",
    "  .limit(10).registerTempTable(\"c_monica\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( My_READ.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"iglesias\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 candidate to a table.\n",
    "  .limit(10).registerTempTable(\"c_iglesias\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( My_READ.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"edmundo\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 candidate to a table.\n",
    "  .limit(10).registerTempTable(\"c_edmundo\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( My_READ.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"monasterio\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 candidate to a table.\n",
    "  .limit(10).registerTempTable(\"c_monasterio\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the table structure\n",
    "from collections import namedtuple\n",
    "fields = (\"tag\", \"SA\" )\n",
    "Tweet = namedtuple( 'Tweet', fields )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(My_READ.map( lambda text: Tweet( text, \"0\" ) ) # Stores in a Tweet Object\n",
    "    .foreachRDD( lambda rdd: rdd.toDF() # Sorts Them in a DF\n",
    "    .limit(20).registerTempTable(\"tweets\") ) # Registers to a table\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "#import seaborn as sns\n",
    "import pandas\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to clean tweets for the SA\n",
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the TweetRead.py file at this point (in the other terminal windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run receive-Tweets.py and after that we can start streaming by running : ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google API Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install google-api-python-client==1.6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install oauth2client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import gspread\n",
    "import pandas as pd\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the scope\n",
    "scope = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# add credentials to the account\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('madridelections2021.json', scope)\n",
    "\n",
    "# authorize the clientsheet \n",
    "client = gspread.authorize(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----  Mentions ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|candidato|count|\n",
      "+---------+-----+\n",
      "|    ayuso|    4|\n",
      "|   ayuso,|    1|\n",
      "|   ayuso!|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from c_ayuso\").show()\n",
    "df_cont = sqlContext.sql(\"select * from c_ayuso\")\n",
    "df_ay = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|       candidato|count|\n",
      "+----------------+-----+\n",
      "|        iglesias|    2|\n",
      "|iglesias.!monica|    1|\n",
      "|     iglesias'rt|    1|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from c_iglesias\").show()\n",
    "df_cont = sqlContext.sql(\"select * from c_iglesias\")\n",
    "df_ig = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|   candidato|count|\n",
      "+------------+-----+\n",
      "|monasterio',|    1|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from c_monasterio\").show()\n",
    "df_cont = sqlContext.sql(\"select * from c_monasterio\")\n",
    "df_ms = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For now we will do only the 3 populars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select * from c_edmundo\").show()\n",
    "df_cont = sqlContext.sql(\"select * from c_edmundo\")\n",
    "df_ed = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select * from c_monica\").show()\n",
    "df_cont = sqlContext.sql(\"select * from c_monica\")\n",
    "df_mo = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select * from c_gabilondo\").show()\n",
    "df_cont = sqlContext.sql(\"select * from c_gabilondo\")\n",
    "df_ga = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all the 6 DF\n",
    "#df_menciones = pd.concat([df_ay, df_ig, df_ga, df_mo, df_ms, df_ed]).drop_duplicates(subset=['candidato'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can append the df and collect in one candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all the 3 DF\n",
    "df_menciones = pd.concat([df_ay, df_ig, df_ms]).drop_duplicates(subset=['candidato'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'ISABEL DIAZ AYUSO' if 'ayuso' in x else x)\n",
    "df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'PABLO IGLESIAS' if 'iglesias' in x else x)\n",
    "df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'ROCIO MONASTERIO' if 'monasterio' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidato</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISABEL DIAZ AYUSO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISABEL DIAZ AYUSO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISABEL DIAZ AYUSO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PABLO IGLESIAS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PABLO IGLESIAS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PABLO IGLESIAS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROCIO MONASTERIO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           candidato  count\n",
       "0  ISABEL DIAZ AYUSO      4\n",
       "1  ISABEL DIAZ AYUSO      1\n",
       "2  ISABEL DIAZ AYUSO      1\n",
       "0     PABLO IGLESIAS      2\n",
       "1     PABLO IGLESIAS      1\n",
       "2     PABLO IGLESIAS      1\n",
       "0   ROCIO MONASTERIO      1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_menciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mentions to Google Sheet LIVE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the instance of the Spreadsheet\n",
    "sheet = client.open('STREAMING-MENTIONS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lote ----->  1\n",
      "Lote ----->  2\n",
      "Lote ----->  3\n",
      "Lote ----->  4\n",
      "Lote ----->  5\n",
      "Lote ----->  6\n",
      "Lote ----->  7\n",
      "Lote ----->  8\n",
      "Lote ----->  9\n",
      "Lote ----->  10\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "df_ant = df_menciones\n",
    "\n",
    "#insert the first query\n",
    "sheet_instance = sheet.get_worksheet(0)\n",
    "sheet_instance.insert_rows(df_menciones.values.tolist(), row = 2)\n",
    "\n",
    "while count < 10:\n",
    "    print(\"Lote -----> \" , count + 1)\n",
    "    time.sleep( 3 )\n",
    "    \n",
    "    ##-----------------Create DF_Mentions----------------------##\n",
    "    df_cont = sqlContext.sql(\"select * from c_ayuso\")\n",
    "    df_ay = df_cont.toPandas()\n",
    "\n",
    "    df_cont = sqlContext.sql(\"select * from c_monasterio\")\n",
    "    df_ms = df_cont.toPandas()\n",
    "\n",
    "    df_cont = sqlContext.sql(\"select * from c_iglesias\")\n",
    "    df_ig = df_cont.toPandas()\n",
    "\n",
    "    # append all the 3 DF\n",
    "    df_menciones = pd.concat([df_ay, df_ig, df_ms]).drop_duplicates(subset=['candidato'])\n",
    "\n",
    "    df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'ISABEL DIAZ AYUSO' if 'ayuso' in x else x)\n",
    "    df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'PABLO IGLESIAS' if 'iglesias' in x else x)\n",
    "    df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'ROCIO MONASTERIO' if 'monasterio' in x else x)\n",
    "    #-----------------END Create DF_Mentions----------------------##\n",
    "    \n",
    "    \n",
    "    if not(df_ant.equals(df_menciones)):\n",
    "        # get the first sheet of the Spreadsheet (SA)\n",
    "        sheet_instance = sheet.get_worksheet(0)\n",
    "        sheet_instance.insert_rows(df_menciones.values.tolist(), row = 2)\n",
    "    \n",
    "    df_ant = df_menciones\n",
    "    count = count + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---- SA ---- Twitts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the instance of the Spreadsheet\n",
    "sheet = client.open('SA-Candidates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                 tag| SA|\n",
      "+--------------------+---+\n",
      "|Â¿Y a Ã©stos tenemo...|  0|\n",
      "|El Gobierno de la...|  0|\n",
      "|                    |  0|\n",
      "|Y se quejaban de ...|  0|\n",
      "|                    |  0|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from tweets\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's define a procedure to clean the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(raw_text):\n",
    "    letters_only = re.sub(\"[^A-zÃ€-Ãº]\", \" \",str(raw_text)) \n",
    "    #print(letters_only)\n",
    "    letters_only = re.sub(\"([HhJj][A-zÃ€-Ãº]){2,}[HhJj]*\", \"jajaja\", str(letters_only))\n",
    "    words = letters_only.lower().split()\n",
    "    #print(words)\n",
    "    stops = set(stopwords.words(\"spanish\"))  \n",
    "    not_stop_words = [w for w in words if not w in stops]\n",
    "    return (\" \".join(not_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can enter in a loop and it will be calculating the SA real-time, getting the tweets from the TABLE SQL \n",
    "that is calculated/produced above - Let's start with 20 - it can be infinite and the dashboard gets constant updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lote ----->  1\n",
      " * Tweet *  -  Y se quejaban de Ayusoâ€¦RT @Emma1492is: AsÃ­ manejan los medios, gente como Vicente VallÃ©s, Ana Rosa, Ferreras, Susana Griso y otros que manipulando y difundiendo bâ€¦RT @Sayo_P75: La anulaciÃ³n de Madrid Central no solo supondrÃ¡ una enorme pÃ©rdida por tener que devolver las multas desde 2019.. tendrÃ¡ un mâ€¦RT @josevico4: Pues parece que Pedro SÃ¡nchez se estarÃ¡ acordando de Pablo Iglesias, ahora ya no hablan del coletas ahora van directos a porâ€¦RT @AgustiColomines: Que aquesta senyora, que forma part del govern espanyol que mantÃ© @jordialapreso a la presÃ³, compari @JuntsXCat amb Ayâ€¦Irene Montero SE REBELA contra Pablo Iglesias https://t.co/mnd1qNGEI7 a travÃ©s de @YouTubeRT @manarellidiego: Y DE DÃ“NDE SALE Ã‰SOS 100 MIL DÃ“LARES QUE PEDRO CASTILLO LE HA PAGADO AL FRACASADO COMUNISTA ESPAÃ‘OL PABLO IGLESIAS PARAâ€¦RT @MuyLiberal: Casado no es Ayuso: \n",
      "The sentiment --->  NEG \n",
      "\n",
      " * Tweet *  -  Tiene en ella el modelo para corregir sus errores, pero lo primero que deberÃ­a hacer es reconocerlosâ€¦RT @JuanSheput: Pablo Iglesias es un polÃ­tico a la medida del comunismo del que se autoproclamÃ³ militante. Denuncia la inequidad y se comprâ€¦RT @EAristeguieta: Hoy me ratificaron que la joyita de Pablo Iglesias se viene a Vzla. Desde ya es persona no grata.RT @pablom_m: Solo vengo para decir que la fiscalÃ­a anticorrupciÃ³n asegura que Rodrigo Rato ocultÃ³ 77 millones de euros en una sociedad deâ€¦RT @And89_3: Pablo Iglesias peleÃ³ la subida del SMI, el IMV o la subida de las pensiones y tÃº le odias, Ana Botella vendiÃ³ viviendas pÃºblicâ€¦RT @josevico4: Pues parece que Pedro SÃ¡nchez se estarÃ¡ acordando de Pablo Iglesias, ahora ya no hablan del coletas ahora van directos a porâ€¦RT @Sayo_P75: La anulaciÃ³n de Madrid Central no solo supondrÃ¡ una enorme pÃ©rdida por tener que devolver las multas desde 2019.. tendrÃ¡ un mâ€¦RT @vayanata: Â¿VolverÃ¡ Pablo Iglesias a pedir guillotina para el rey o llamar terroristas a Amancio Ortega ahora que ya no tiene que disimuâ€¦RT @Carl0smoreno: Se puliÃ³ el dinero del estado y de los fondos COVID alimentando su megalomania en vez de gastarlo en reforzar la sanidad,â€¦RT @WillyTolerdoo: - No hay cojones de que el gobierno balear culpe a Ayuso del veto de Reino Unido a Baleares.\n",
      "The sentiment --->  NEG \n",
      "\n",
      " * Tweet *  -  - SujÃ©tame el cubata ðŸ¥ƒRT @jm_clavero: El Constitucional se inclina por anular la designaciÃ³n de Pablo Iglesias para el Ã³rgano rector del CNI\n",
      "The sentiment --->  NEG \n",
      "\n",
      " * Tweet *  -  Mucha prisa se han dâ€¦RT @pablom_m: Solo vengo para decir que la fiscalÃ­a anticorrupciÃ³n asegura que Rodrigo Rato ocultÃ³ 77 millones de euros en una sociedad deâ€¦RT @JC_C_A: MartÃ­nez Almeida anula la zona libre de emisiones de Madrid Central y DÃ­az Ayuso recupera \"la identidad de Madrid, los atascosâ€¦RT @Pableraso: La firma del hospital de campaÃ±a demandarÃ¡ a la conselleria de Sanidad por una deuda de 6,5 millones\n",
      "The sentiment --->  NEG \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_content = sqlContext.sql( 'Select tag, SA from tweets' )\n",
    "df1 = df1_content.toPandas()\n",
    "\n",
    "    \n",
    "count = 0\n",
    "\n",
    "while count < 1:\n",
    "    print(\"Lote -----> \" , count + 1)\n",
    "    time.sleep( 3 )\n",
    "    df2_content = sqlContext.sql( 'Select tag, SA from tweets' )\n",
    "    df2 = df2_content.toPandas()\n",
    "    \n",
    "    # Concatenating dataframes without duplicates\n",
    "    df_tweets = pd.concat([df1, df2]).drop_duplicates(subset=['tag'])\n",
    "    df_tweets = df_tweets[df_tweets['tag'].apply(len)>10]\n",
    "\n",
    "    df_tweets['SA'] = \"\"\n",
    "    i = 0\n",
    "    \n",
    "    for i, row in df_tweets.iterrows():\n",
    "        print(\" * Tweet *  - \" , df_tweets[\"tag\"][i])\n",
    "        twit = df_tweets[\"tag\"][i]\n",
    "        twit = process_text(twit)\n",
    "        sa = analyzer.predict(twit)\n",
    "        df_tweets.at[i,'SA'] = sa\n",
    "        print(\"The sentiment ---> \", sa, \"\\n\")\n",
    "    \n",
    "    df=df_tweets.copy()\n",
    "    sheet_instance = sheet.get_worksheet(0)\n",
    "    sheet_instance.insert_rows(df.values.tolist(), row = 2)\n",
    "    count = count + 1\n",
    "\n",
    "    # get the first sheet of the Spreadsheet (SA)\n",
    "\n",
    "\n",
    "    df1 = df_tweets.copy()\n",
    "    df1['SA'] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inside the loop we have the DF (df_tweets) that is being calculated.\n",
    "## At the end of the loop, we will have the last DF with the SA to be published in Tablaeu\n",
    "## This will be refreshed everytime that we run the LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df_tweets.copy()\n",
    "#sheet_instance = sheet.get_worksheet(0)\n",
    "#sheet_instance.insert_rows(df.values.tolist(), row = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets.sort_values(by=['tag'])\n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
