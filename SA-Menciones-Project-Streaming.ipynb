{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://qs.topuniversities.com/hs-fs/hubfs/07_HumanScience_H_300.jpg?width=4961&height=3508&name=07_HumanScience_H_300.jpg\" width=\"250\" height=\"100\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk The Talk || Hands-On Big Data Streaming using Apache Spark \n",
    "## Real-Time Analysis of Your Political Standing\n",
    "### Group F\n",
    "#### Nicolas Fraire, Camille Eloi, Iñigo Hidalgo, Dea Markovic, Harshil Sharma, Oriol Vila, Gerardo Gandara\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the amazing frameworks that can handle big data in real-time and perform different analysis, is Apache Spark. I think there are many resources that provide information about different features of Spark and how popular it is in the big data community but shortly mentioning the core features of Spark: it does fast big data processing employing Resilient Distributed Datasets (RDDs), streaming and Machine learning on a large scale at real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/07/performing-twitter-sentiment-analysis1.jpg\" />\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade pip\n",
    "#!pip3 install pysentimiento\n",
    "#!pip3 install torch\n",
    "#!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentimiento import SentimentAnalyzer\n",
    "analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We are going to use findspark library to locate Spark on our local machine and then we import necessary packages from pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you need to put where is spark installed\n",
    "# with this command : echo 'sc.getConfget('spark.home')' | spark-shell\n",
    "findspark.init('/opt/spark-3.0.0-bin-hadoop3.2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May cause deprecation warnings, safe to ignore, they aren't errors\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We initiate SparkContext(). SparkContext is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster. SparkContext here uses Py4J to launch a JVM and creates a JavaSparkContext (source). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can only run this once. restart your kernel for any errors.\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is important to note that **only one SparkContext** may be running at each session. (That is why the trial and error is time consuming when you are learning, but it is ok, it is worthy!).\n",
    "\n",
    "After that, we initiate the StreamingContext() with 10-second batch intervals, it means that the input streams will be divided into batches every 10 seconds during the streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10 )\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 5555)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.checkpoint(\"checkpoint_TwitterApp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, our next step is to assign our input source of streaming and then put the incoming data in variable lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is important to note that we are using the same port number (5555) as we used in the first module to send the tweets and the IP address (VM for our case) is the same since we are running things on our local machine. in addition\n",
    "\n",
    "**We are using the window() function to determine that we are analyzing tweets every minute (60 seconds) to see what the top 10 #tags are during that time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = socket_stream.window( 60 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will create TEMP tables for each candidate/mentions and a table with TWEETS for SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politicians Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we need to calculate how many times the Candidate has been mentioned. We can do that by using the function reduceByKey. This function will calculate how many times the hashtag has been mentioned per each batch, i.e. it will reset the counts in each batch. In this version for prototype purposes we used reduceByKey but it important to emphasize the difference. We can simulate it with a loop calculation later for you to see the difference.\n",
    "\n",
    "#### In our case, we need to calculate the counts across all the batches, so we’ll use another function called updateStateByKey, as this function allows you to maintain the state of RDD while updating it with new data. This way is called Stateful Transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( lines.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"ayuso\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 key words that starts with the candidate name to a table.\n",
    "  .limit(10).registerTempTable(\"c_ayuso\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( lines.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"gabilondo\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 candidate to a table.\n",
    "  .limit(10).registerTempTable(\"c_gabilondo\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( lines.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"monica\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 candidate to a table.\n",
    "  .limit(10).registerTempTable(\"c_monica\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( lines.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"iglesias\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 candidate to a table.\n",
    "  .limit(10).registerTempTable(\"c_iglesias\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( lines.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"edmundo\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 candidate to a table.\n",
    "  .limit(10).registerTempTable(\"c_edmundo\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( lines.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"monasterio\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 candidate to a table.\n",
    "  .limit(10).registerTempTable(\"c_monasterio\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the table structure\n",
    "from collections import namedtuple\n",
    "fields = (\"tag\", \"SA\" )\n",
    "Tweet = namedtuple( 'Tweet', fields )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lines.map( lambda text: Tweet( text, \"0\" ) ) # Stores in a Tweet Object\n",
    "    .foreachRDD( lambda rdd: rdd.toDF() # Sorts Them in a DF\n",
    "    .limit(20).registerTempTable(\"tweets\") ) # Registers to a table\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "#import seaborn as sns\n",
    "import pandas\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to clean tweets for the SA\n",
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the TweetRead.py file at this point (in the other terminal windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run receive-Tweets.py and after that we can start streaming by running : ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google API Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install google-api-python-client==1.6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install oauth2client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import gspread\n",
    "import pandas as pd\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the scope\n",
    "scope = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# add credentials to the account\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('madridelections2021.json', scope)\n",
    "\n",
    "# authorize the clientsheet \n",
    "client = gspread.authorize(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----  Mentions ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select * from c_ayuso\").show()\n",
    "df_cont = sqlContext.sql(\"select * from c_ayuso\")\n",
    "df_ay = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select * from c_iglesias\").show()\n",
    "df_cont = sqlContext.sql(\"select * from c_iglesias\")\n",
    "df_ig = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select * from c_monasterio\").show()\n",
    "df_cont = sqlContext.sql(\"select * from c_monasterio\")\n",
    "df_ms = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For now we will do only the 3 populars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlContext.sql(\"select * from c_edmundo\").show()\n",
    "#df_cont = sqlContext.sql(\"select * from c_edmundo\")\n",
    "#df_ed = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlContext.sql(\"select * from c_monica\").show()\n",
    "#df_cont = sqlContext.sql(\"select * from c_monica\")\n",
    "#df_mo = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlContext.sql(\"select * from c_gabilondo\").show()\n",
    "#df_cont = sqlContext.sql(\"select * from c_gabilondo\")\n",
    "#df_ga = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all the 6 DF\n",
    "#df_menciones = pd.concat([df_ay, df_ig, df_ga, df_mo, df_ms, df_ed]).drop_duplicates(subset=['candidato'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can append the df and collect in one candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all the 3 DF\n",
    "df_menciones = pd.concat([df_ay, df_ig, df_ms]).drop_duplicates(subset=['candidato'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'ISABEL DIAZ AYUSO' if 'ayuso' in x else x)\n",
    "df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'PABLO IGLESIAS' if 'iglesias' in x else x)\n",
    "df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'ROCIO MONASTERIO' if 'monasterio' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_menciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mentions to Google Sheet LIVE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the instance of the Spreadsheet\n",
    "sheet = client.open('STREAMING-MENTIONS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "df_ant = df_menciones\n",
    "\n",
    "#insert the first query\n",
    "sheet_instance = sheet.get_worksheet(0)\n",
    "sheet_instance.insert_rows(df_menciones.values.tolist(), row = 2)\n",
    "\n",
    "while count < 5:\n",
    "    print(\"Lote -----> \" , count + 1)\n",
    "    time.sleep( 3 )\n",
    "    \n",
    "    ##-----------------Create DF_Mentions----------------------##\n",
    "    df_cont = sqlContext.sql(\"select * from c_ayuso\")\n",
    "    df_ay = df_cont.toPandas()\n",
    "\n",
    "    df_cont = sqlContext.sql(\"select * from c_monasterio\")\n",
    "    df_ms = df_cont.toPandas()\n",
    "\n",
    "    df_cont = sqlContext.sql(\"select * from c_iglesias\")\n",
    "    df_ig = df_cont.toPandas()\n",
    "\n",
    "    # append all the 3 DF\n",
    "    df_menciones = pd.concat([df_ay, df_ig, df_ms]).drop_duplicates(subset=['candidato'])\n",
    "\n",
    "    df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'ISABEL DIAZ AYUSO' if 'ayuso' in x else x)\n",
    "    df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'PABLO IGLESIAS' if 'iglesias' in x else x)\n",
    "    df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'ROCIO MONASTERIO' if 'monasterio' in x else x)\n",
    "    #-----------------END Create DF_Mentions----------------------##\n",
    "    \n",
    "    \n",
    "    if not(df_ant.equals(df_menciones)):\n",
    "        # get the first sheet of the Spreadsheet (SA)\n",
    "        sheet_instance = sheet.get_worksheet(0)\n",
    "        sheet_instance.insert_rows(df_menciones.values.tolist(), row = 2)\n",
    "    \n",
    "    df_ant = df_menciones\n",
    "    count = count + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---- SA ---- Twitts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the instance of the Spreadsheet\n",
    "sheet = client.open('SA-Candidates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select * from tweets\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's define a procedure to clean the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(raw_text):\n",
    "    letters_only = re.sub(\"[^A-zÀ-ú]\", \" \",str(raw_text)) \n",
    "    #print(letters_only)\n",
    "    letters_only = re.sub(\"([HhJj][A-zÀ-ú]){2,}[HhJj]*\", \"jajaja\", str(letters_only))\n",
    "    words = letters_only.lower().split()\n",
    "    #print(words)\n",
    "    stops = set(stopwords.words(\"spanish\"))  \n",
    "    not_stop_words = [w for w in words if not w in stops]\n",
    "    return (\" \".join(not_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can enter in a loop and it will be calculating the SA real-time, getting the tweets from the TABLE SQL \n",
    "that is calculated/produced above - Let's start with 20 - it can be infinite and the dashboard gets constant updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_content = sqlContext.sql( 'Select tag, SA from tweets' )\n",
    "df1 = df1_content.toPandas()\n",
    "\n",
    "    \n",
    "count = 0\n",
    "\n",
    "while count < 1:\n",
    "    print(\"Lote -----> \" , count + 1)\n",
    "    time.sleep( 3 )\n",
    "    df2_content = sqlContext.sql( 'Select tag, SA from tweets' )\n",
    "    df2 = df2_content.toPandas()\n",
    "    \n",
    "    # Concatenating dataframes without duplicates\n",
    "    df_tweets = pd.concat([df1, df2]).drop_duplicates(subset=['tag'])\n",
    "    df_tweets = df_tweets[df_tweets['tag'].apply(len)>10]\n",
    "\n",
    "    df_tweets['SA'] = \"\"\n",
    "    i = 0\n",
    "    \n",
    "    for i, row in df_tweets.iterrows():\n",
    "        print(\" * Tweet *  - \" , df_tweets[\"tag\"][i])\n",
    "        twit = df_tweets[\"tag\"][i]\n",
    "        twit = process_text(twit)\n",
    "        sa = analyzer.predict(twit)\n",
    "        df_tweets.at[i,'SA'] = sa\n",
    "        print(\"The sentiment ---> \", sa, \"\\n\")\n",
    "    \n",
    "    df=df_tweets.copy()\n",
    "    sheet_instance = sheet.get_worksheet(0)\n",
    "    sheet_instance.insert_rows(df.values.tolist(), row = 2)\n",
    "    count = count + 1\n",
    "\n",
    "    # get the first sheet of the Spreadsheet (SA)\n",
    "\n",
    "\n",
    "    df1 = df_tweets.copy()\n",
    "    df1['SA'] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inside the loop we have the DF (df_tweets) that is being calculated.\n",
    "## At the end of the loop, we will have the last DF with the SA to be published in Tablaeu\n",
    "## This will be refreshed everytime that we run the LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df_tweets.copy()\n",
    "#sheet_instance = sheet.get_worksheet(0)\n",
    "#sheet_instance.insert_rows(df.values.tolist(), row = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets.sort_values(by=['tag'])\n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
