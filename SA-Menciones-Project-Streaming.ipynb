{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://qs.topuniversities.com/hs-fs/hubfs/07_HumanScience_H_300.jpg?width=4961&height=3508&name=07_HumanScience_H_300.jpg\" width=\"250\" height=\"100\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk The Talk || Hands-On Big Data Streaming using Apache Spark \n",
    "## Real-Time Analysis of Your Political Standing\n",
    "### Group F\n",
    "#### Nicolas Fraire, Camille Eloi, Iñigo Hidalgo, Dea Markovic, Harshil Sharma, Oriol Vila, Gerardo Gandara\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the amazing frameworks that can handle big data in real-time and perform different analysis, is Apache Spark. I think there are many resources that provide information about different features of Spark and how popular it is in the big data community but shortly mentioning the core features of Spark: it does fast big data processing employing Resilient Distributed Datasets (RDDs), streaming and Machine learning on a large scale at real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/07/performing-twitter-sentiment-analysis1.jpg" />\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade pip\n",
    "#!pip3 install pysentimiento\n",
    "#!pip3 install torch\n",
    "#!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentimiento import SentimentAnalyzer\n",
    "analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We are going to use findspark library to locate Spark on our local machine and then we import necessary packages from pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you need to put where is spark installed\n",
    "# with this command : echo 'sc.getConfget('spark.home')' | spark-shell\n",
    "findspark.init('/opt/spark-3.0.0-bin-hadoop3.2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May cause deprecation warnings, safe to ignore, they aren't errors\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We initiate SparkContext(). SparkContext is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster. SparkContext here uses Py4J to launch a JVM and creates a JavaSparkContext (source). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can only run this once. restart your kernel for any errors.\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is important to note that **only one SparkContext** may be running at each session. (That is why the trial and error is time consuming when you are learning, but it is ok, it is worthy!).\n",
    "\n",
    "After that, we initiate the StreamingContext() with 10-second batch intervals, it means that the input streams will be divided into batches every 10 seconds during the streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10 )\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 5555)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.checkpoint(\"checkpoint_TwitterApp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, our next step is to assign our input source of streaming and then put the incoming data in variable My_READ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is important to note that we are using the same port number (5555) as we used in the first module to send the tweets and the IP address (VM for our case) is the same since we are running things on our local machine. in addition\n",
    "\n",
    "**We are using the window() function to determine that we are analyzing tweets every minute (60 seconds) to see what the top 10 #tags are during that time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_READ = socket_stream.window( 60 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will create TEMP tables for each candidate/mentions and a table with TWEETS for SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politicians Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we need to calculate how many times the Candidate has been mentioned. We can do that by using the function reduceByKey. This function will calculate how many times the hashtag has been mentioned per each batch, i.e. it will reset the counts in each batch. In this version for prototype purposes we used reduceByKey but it important to emphasize the difference. We can simulate it with a loop calculation later for you to see the difference.\n",
    "\n",
    "#### In our case, we need to calculate the counts across all the batches, so we’ll use another function called updateStateByKey, as this function allows you to maintain the state of RDD while updating it with new data. This way is called Stateful Transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( My_READ.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"ayuso\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 key words that starts with the candidate name to a table.\n",
    "  .limit(10).registerTempTable(\"c_ayuso\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( My_READ.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"gabilondo\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 candidate to a table.\n",
    "  .limit(10).registerTempTable(\"c_gabilondo\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( My_READ.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"monica\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 candidate to a table.\n",
    "  .limit(10).registerTempTable(\"c_monica\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( My_READ.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"iglesias\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 candidate to a table.\n",
    "  .limit(10).registerTempTable(\"c_iglesias\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( My_READ.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"edmundo\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 candidate to a table.\n",
    "  .limit(10).registerTempTable(\"c_edmundo\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tuple to assign names\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"candidato\", \"count\" )\n",
    "Candidato = namedtuple( 'Candidato', fields )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "\n",
    "( My_READ.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    candidate calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"monasterio\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Candidato( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 candidate to a table.\n",
    "  .limit(10).registerTempTable(\"c_monasterio\") ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the table structure\n",
    "from collections import namedtuple\n",
    "fields = (\"tag\", \"SA\" )\n",
    "Tweet = namedtuple( 'Tweet', fields )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(My_READ.map( lambda text: Tweet( text, \"0\" ) ) # Stores in a Tweet Object\n",
    "    .foreachRDD( lambda rdd: rdd.toDF() # Sorts Them in a DF\n",
    "    .limit(20).registerTempTable(\"tweets\") ) # Registers to a table\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "#import seaborn as sns\n",
    "import pandas\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to clean tweets for the SA\n",
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the TweetRead.py file at this point (in the other terminal windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run receive-Tweets.py and after that we can start streaming by running : ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google API Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install google-api-python-client==1.6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install oauth2client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import gspread\n",
    "import pandas as pd\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the scope\n",
    "scope = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# add credentials to the account\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('madridelections2021.json', scope)\n",
    "\n",
    "# authorize the clientsheet \n",
    "client = gspread.authorize(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----  Mentions ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|candidato|count|\n",
      "+---------+-----+\n",
      "|    ayuso|    4|\n",
      "|   ayuso,|    1|\n",
      "|   ayuso!|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from c_ayuso\").show()\n",
    "df_cont = sqlContext.sql(\"select * from c_ayuso\")\n",
    "df_ay = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|       candidato|count|\n",
      "+----------------+-----+\n",
      "|        iglesias|    2|\n",
      "|iglesias.!monica|    1|\n",
      "|     iglesias'rt|    1|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from c_iglesias\").show()\n",
    "df_cont = sqlContext.sql(\"select * from c_iglesias\")\n",
    "df_ig = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|   candidato|count|\n",
      "+------------+-----+\n",
      "|monasterio',|    1|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from c_monasterio\").show()\n",
    "df_cont = sqlContext.sql(\"select * from c_monasterio\")\n",
    "df_ms = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For now we will do only the 3 populars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select * from c_edmundo\").show()\n",
    "df_cont = sqlContext.sql(\"select * from c_edmundo\")\n",
    "df_ed = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select * from c_monica\").show()\n",
    "df_cont = sqlContext.sql(\"select * from c_monica\")\n",
    "df_mo = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select * from c_gabilondo\").show()\n",
    "df_cont = sqlContext.sql(\"select * from c_gabilondo\")\n",
    "df_ga = df_cont.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all the 6 DF\n",
    "#df_menciones = pd.concat([df_ay, df_ig, df_ga, df_mo, df_ms, df_ed]).drop_duplicates(subset=['candidato'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can append the df and collect in one candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all the 3 DF\n",
    "df_menciones = pd.concat([df_ay, df_ig, df_ms]).drop_duplicates(subset=['candidato'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'ISABEL DIAZ AYUSO' if 'ayuso' in x else x)\n",
    "df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'PABLO IGLESIAS' if 'iglesias' in x else x)\n",
    "df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'ROCIO MONASTERIO' if 'monasterio' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidato</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISABEL DIAZ AYUSO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISABEL DIAZ AYUSO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISABEL DIAZ AYUSO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PABLO IGLESIAS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PABLO IGLESIAS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PABLO IGLESIAS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROCIO MONASTERIO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           candidato  count\n",
       "0  ISABEL DIAZ AYUSO      4\n",
       "1  ISABEL DIAZ AYUSO      1\n",
       "2  ISABEL DIAZ AYUSO      1\n",
       "0     PABLO IGLESIAS      2\n",
       "1     PABLO IGLESIAS      1\n",
       "2     PABLO IGLESIAS      1\n",
       "0   ROCIO MONASTERIO      1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_menciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mentions to Google Sheet LIVE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the instance of the Spreadsheet\n",
    "sheet = client.open('STREAMING-MENTIONS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lote ----->  1\n",
      "Lote ----->  2\n",
      "Lote ----->  3\n",
      "Lote ----->  4\n",
      "Lote ----->  5\n",
      "Lote ----->  6\n",
      "Lote ----->  7\n",
      "Lote ----->  8\n",
      "Lote ----->  9\n",
      "Lote ----->  10\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "df_ant = df_menciones\n",
    "\n",
    "#insert the first query\n",
    "sheet_instance = sheet.get_worksheet(0)\n",
    "sheet_instance.insert_rows(df_menciones.values.tolist(), row = 2)\n",
    "\n",
    "while count < 10:\n",
    "    print(\"Lote -----> \" , count + 1)\n",
    "    time.sleep( 3 )\n",
    "    \n",
    "    ##-----------------Create DF_Mentions----------------------##\n",
    "    df_cont = sqlContext.sql(\"select * from c_ayuso\")\n",
    "    df_ay = df_cont.toPandas()\n",
    "\n",
    "    df_cont = sqlContext.sql(\"select * from c_monasterio\")\n",
    "    df_ms = df_cont.toPandas()\n",
    "\n",
    "    df_cont = sqlContext.sql(\"select * from c_iglesias\")\n",
    "    df_ig = df_cont.toPandas()\n",
    "\n",
    "    # append all the 3 DF\n",
    "    df_menciones = pd.concat([df_ay, df_ig, df_ms]).drop_duplicates(subset=['candidato'])\n",
    "\n",
    "    df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'ISABEL DIAZ AYUSO' if 'ayuso' in x else x)\n",
    "    df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'PABLO IGLESIAS' if 'iglesias' in x else x)\n",
    "    df_menciones.candidato = df_menciones.candidato.apply(lambda x: 'ROCIO MONASTERIO' if 'monasterio' in x else x)\n",
    "    #-----------------END Create DF_Mentions----------------------##\n",
    "    \n",
    "    \n",
    "    if not(df_ant.equals(df_menciones)):\n",
    "        # get the first sheet of the Spreadsheet (SA)\n",
    "        sheet_instance = sheet.get_worksheet(0)\n",
    "        sheet_instance.insert_rows(df_menciones.values.tolist(), row = 2)\n",
    "    \n",
    "    df_ant = df_menciones\n",
    "    count = count + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---- SA ---- Twitts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the instance of the Spreadsheet\n",
    "sheet = client.open('SA-Candidates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                 tag| SA|\n",
      "+--------------------+---+\n",
      "|¿Y a éstos tenemo...|  0|\n",
      "|El Gobierno de la...|  0|\n",
      "|                    |  0|\n",
      "|Y se quejaban de ...|  0|\n",
      "|                    |  0|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from tweets\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's define a procedure to clean the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(raw_text):\n",
    "    letters_only = re.sub(\"[^A-zÀ-ú]\", \" \",str(raw_text)) \n",
    "    #print(letters_only)\n",
    "    letters_only = re.sub(\"([HhJj][A-zÀ-ú]){2,}[HhJj]*\", \"jajaja\", str(letters_only))\n",
    "    words = letters_only.lower().split()\n",
    "    #print(words)\n",
    "    stops = set(stopwords.words(\"spanish\"))  \n",
    "    not_stop_words = [w for w in words if not w in stops]\n",
    "    return (\" \".join(not_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can enter in a loop and it will be calculating the SA real-time, getting the tweets from the TABLE SQL \n",
    "that is calculated/produced above - Let's start with 20 - it can be infinite and the dashboard gets constant updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lote ----->  1\n",
      " * Tweet *  -  Y se quejaban de Ayuso…RT @Emma1492is: Así manejan los medios, gente como Vicente Vallés, Ana Rosa, Ferreras, Susana Griso y otros que manipulando y difundiendo b…RT @Sayo_P75: La anulación de Madrid Central no solo supondrá una enorme pérdida por tener que devolver las multas desde 2019.. tendrá un m…RT @josevico4: Pues parece que Pedro Sánchez se estará acordando de Pablo Iglesias, ahora ya no hablan del coletas ahora van directos a por…RT @AgustiColomines: Que aquesta senyora, que forma part del govern espanyol que manté @jordialapreso a la presó, compari @JuntsXCat amb Ay…Irene Montero SE REBELA contra Pablo Iglesias https://t.co/mnd1qNGEI7 a través de @YouTubeRT @manarellidiego: Y DE DÓNDE SALE ÉSOS 100 MIL DÓLARES QUE PEDRO CASTILLO LE HA PAGADO AL FRACASADO COMUNISTA ESPAÑOL PABLO IGLESIAS PARA…RT @MuyLiberal: Casado no es Ayuso: \n",
      "The sentiment --->  NEG \n",
      "\n",
      " * Tweet *  -  Tiene en ella el modelo para corregir sus errores, pero lo primero que debería hacer es reconocerlos…RT @JuanSheput: Pablo Iglesias es un político a la medida del comunismo del que se autoproclamó militante. Denuncia la inequidad y se compr…RT @EAristeguieta: Hoy me ratificaron que la joyita de Pablo Iglesias se viene a Vzla. Desde ya es persona no grata.RT @pablom_m: Solo vengo para decir que la fiscalía anticorrupción asegura que Rodrigo Rato ocultó 77 millones de euros en una sociedad de…RT @And89_3: Pablo Iglesias peleó la subida del SMI, el IMV o la subida de las pensiones y tú le odias, Ana Botella vendió viviendas públic…RT @josevico4: Pues parece que Pedro Sánchez se estará acordando de Pablo Iglesias, ahora ya no hablan del coletas ahora van directos a por…RT @Sayo_P75: La anulación de Madrid Central no solo supondrá una enorme pérdida por tener que devolver las multas desde 2019.. tendrá un m…RT @vayanata: ¿Volverá Pablo Iglesias a pedir guillotina para el rey o llamar terroristas a Amancio Ortega ahora que ya no tiene que disimu…RT @Carl0smoreno: Se pulió el dinero del estado y de los fondos COVID alimentando su megalomania en vez de gastarlo en reforzar la sanidad,…RT @WillyTolerdoo: - No hay cojones de que el gobierno balear culpe a Ayuso del veto de Reino Unido a Baleares.\n",
      "The sentiment --->  NEG \n",
      "\n",
      " * Tweet *  -  - Sujétame el cubata 🥃RT @jm_clavero: El Constitucional se inclina por anular la designación de Pablo Iglesias para el órgano rector del CNI\n",
      "The sentiment --->  NEG \n",
      "\n",
      " * Tweet *  -  Mucha prisa se han d…RT @pablom_m: Solo vengo para decir que la fiscalía anticorrupción asegura que Rodrigo Rato ocultó 77 millones de euros en una sociedad de…RT @JC_C_A: Martínez Almeida anula la zona libre de emisiones de Madrid Central y Díaz Ayuso recupera \"la identidad de Madrid, los atascos…RT @Pableraso: La firma del hospital de campaña demandará a la conselleria de Sanidad por una deuda de 6,5 millones\n",
      "The sentiment --->  NEG \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_content = sqlContext.sql( 'Select tag, SA from tweets' )\n",
    "df1 = df1_content.toPandas()\n",
    "\n",
    "    \n",
    "count = 0\n",
    "\n",
    "while count < 1:\n",
    "    print(\"Lote -----> \" , count + 1)\n",
    "    time.sleep( 3 )\n",
    "    df2_content = sqlContext.sql( 'Select tag, SA from tweets' )\n",
    "    df2 = df2_content.toPandas()\n",
    "    \n",
    "    # Concatenating dataframes without duplicates\n",
    "    df_tweets = pd.concat([df1, df2]).drop_duplicates(subset=['tag'])\n",
    "    df_tweets = df_tweets[df_tweets['tag'].apply(len)>10]\n",
    "\n",
    "    df_tweets['SA'] = \"\"\n",
    "    i = 0\n",
    "    \n",
    "    for i, row in df_tweets.iterrows():\n",
    "        print(\" * Tweet *  - \" , df_tweets[\"tag\"][i])\n",
    "        twit = df_tweets[\"tag\"][i]\n",
    "        twit = process_text(twit)\n",
    "        sa = analyzer.predict(twit)\n",
    "        df_tweets.at[i,'SA'] = sa\n",
    "        print(\"The sentiment ---> \", sa, \"\\n\")\n",
    "    \n",
    "    df=df_tweets.copy()\n",
    "    sheet_instance = sheet.get_worksheet(0)\n",
    "    sheet_instance.insert_rows(df.values.tolist(), row = 2)\n",
    "    count = count + 1\n",
    "\n",
    "    # get the first sheet of the Spreadsheet (SA)\n",
    "\n",
    "\n",
    "    df1 = df_tweets.copy()\n",
    "    df1['SA'] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inside the loop we have the DF (df_tweets) that is being calculated.\n",
    "## At the end of the loop, we will have the last DF with the SA to be published in Tablaeu\n",
    "## This will be refreshed everytime that we run the LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df_tweets.copy()\n",
    "#sheet_instance = sheet.get_worksheet(0)\n",
    "#sheet_instance.insert_rows(df.values.tolist(), row = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets.sort_values(by=['tag'])\n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
